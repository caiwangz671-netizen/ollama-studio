# ollama-studio

Ollama Studio 是一个本地运行的 Web 客户端，用于连接 Ollama 服务并进行对话。
它提供流式生成、RAG 知识库、工具调用、多模态输入以及完整的对话管理能力。
项目定位为“可视化的一体化本地推理工作台”，强调可控、可扩展和低依赖。

---

## 关键词

- 本地推理
- 流式对话
- RAG 检索
- 多模态协作
- 工具调用
- 显存估算

---

## 功能总览

- 本地模型对话与流式响应
- Markdown 渲染与代码高亮
- 思考过程折叠展示与耗时统计
- RAG 知识库管理与检索
- 内置联网搜索工具
- 显存估算与折叠展示
- 多模态输入：文本、语音、图片、音频、文档
- 附件预览、文档内容拼接进上下文
- 会话列表、自动标题与搜索
- 状态提示与工具调用状态提示

---

## 运行环境

- Python 3.x
- Ollama 服务已启动
- 现代浏览器（建议使用 Chrome 或 Edge）

---

## 快速开始

### 1. 启动 Ollama 服务

默认地址为：

```
http://127.0.0.1:11434
```

确保 Ollama 已运行并已下载至少一个模型。

### 2. 启动本地服务

在项目目录内执行：

```bash
python3 server.py
```

### 3. 打开浏览器访问

```
http://localhost:8000
```

### 4. 使用自定义 Ollama 地址

如需自定义 Ollama 地址，设置环境变量：

```bash
export OLLAMA_BASE_URL=http://127.0.0.1:11434
```

---

## 目标用户

- 需要本地部署推理并希望有图形化界面的人
- 希望进行“文本 + 附件 + 语音”混合输入的开发者
- 有知识库需求（RAG）并希望在对话中使用检索增强的人
- 需要更直观地管理对话与模型参数的人
- 需要在本地进行工具调用与联网搜索集成的人

---

## 页面结构概览

### 左侧栏

- 模型选择下拉列表
- 模型扫描按钮
- 历史对话列表与搜索框
- 知识库入口
- 设置入口

### 主对话区域

- 顶部显示当前模型
- 状态提示：就绪 / 生成中 / 思考中
- 中间为对话流展示
- 底部为输入区与附件栏

### 右侧显存估算卡片

- 根据模型名与上下文长度估算显存
- 支持折叠显示，减少界面干扰

---

## 多模态输入与输出

### 支持的输入类型

- 文本输入
- 语音输入（浏览器语音识别）
- 图片上传
- 音频上传
- 文档上传（文本、Markdown、JSON、CSV、PDF）

### 使用方式

- 点击输入框左侧的附件按钮添加文件
- 直接拖拽文件到输入框区域
- 点击麦克风按钮进行语音输入

### 处理策略

- 语音输入会被转为文本并拼接进输入框
- 图片附件以 base64 形式传入模型
- 文档类附件若为文本格式，会拼接到上下文
- 非文本附件提供预览与下载链接
- 单个附件限制 10MB

---

## 主要能力详解

### 1. 流式对话

- 基于 /api/chat 的流式响应
- 前端逐行解析并更新消息内容
- 适配连续输出的流式 UI 体验

### 2. Markdown 渲染

- 内置 Markdown 渲染器
- 支持标题、列表、引用、表格、链接
- 代码块支持语言标识与自动识别
- 语法高亮基于 highlight.js

### 3. RAG 知识库

- 支持知识新增、删除、更新、清空
- 前端可搜索、筛选与批量删除
- 每次对话可自动检索并注入上下文

### 4. 工具调用

- 内置联网搜索工具
- 可根据用户输入决定是否启用工具
- 工具调用结果会写入对话上下文

### 5. 显存估算(不成熟）

- 根据模型名识别参数规模
- 结合量化与上下文长度估算显存
- 适合快速判断设备可运行性

### 6. 附件渲染

- 图片在对话中显示缩略图
- 音频支持原生播放
- 文档支持预览与下载

---

## 设置面板说明

### 常规设置

- Ollama 服务地址

### 生成参数

- 系统提示词
- 上下文长度（num_ctx）
- 最大输出 Token
- 温度、Top P、重复惩罚
- 随机种子

### 知识库与工具

- 是否启用 RAG
- 检索阈值与返回数量
- 是否启用联网搜索工具

---

## 对话流程详解

1. 用户输入文字或附件
2. 前端构造消息对象
3. 可选触发 RAG 检索
4. 组合系统提示词与历史消息
5. 启动流式请求
6. 解析响应并渲染 UI
7. 若有工具调用，进入工具执行流程

---

## 工具调用流程详解

1. 判断是否提供工具（基于用户输入）
2. 将工具描述注入模型请求
3. 模型返回 tool_calls
4. 前端执行工具并写入 tool role 消息
5. 继续下一轮生成直到完成

---

## RAG 处理流程

1. 解析用户输入与附件内容
2. 调用 /api/rag/query 获取相关记忆
3. 拼接检索结果到系统提示中
4. 发送最终消息到模型进行生成

---

## API 端点说明

### GET /api/models

- 获取 Ollama 模型列表

### GET /api/models/tool-capable

- 返回一个支持工具调用的推荐模型

### GET /api/rag/status

- 返回可用 embedding 模型状态

### GET /api/rag/memories

- 获取所有知识库记录

### POST /api/chat

- 与 Ollama 交互，支持流式响应

### POST /api/rag/query

- 查询知识库，返回匹配结果

### POST /api/rag/add

- 添加知识库记录

### POST /api/rag/update

- 更新知识库记录

### POST /api/rag/delete

- 删除知识库记录

### POST /api/rag/clear

- 清空知识库记录

### POST /api/tools/web_search

- 联网搜索工具接口

---

## 数据存储说明

- 知识库数据存储在 `memory.db`
- 聊天记录存储在浏览器 LocalStorage
- 附件数据仅在内存中处理
- 页面刷新后附件会被清空

---

## 语音能力说明

- 依赖浏览器 SpeechRecognition API
- 语音会被转为文本输入框内容
- 若浏览器不支持会提示不可用

---

## 附件能力说明

### 文档

- 文本类文档会提取内容进入上下文
- 非文本文档提供下载链接

### 图片

- 图片转为 base64 并注入消息 payload
- 前端显示缩略图预览

### 音频

- 音频提供可播放控件
- 不会自动转写

---

## 对话管理

- 支持新建对话
- 支持删除对话
- 支持自动生成标题
- 支持手动重命名
- 支持历史搜索

---

## 显存估算说明

- 通过模型名称识别参数规模
- 结合量化级别估算权重占用
- 结合上下文长度估算 KV Cache
- 加入运行时开销与安全余量

---

## 知识库管理说明

- 支持单条新增
- 支持编辑内容
- 支持批量删除
- 支持清空所有记录
- 支持导出为 JSON

---

## UI 交互说明

- Enter 发送，Shift+Enter 换行
- 思考过程可手动展开/收起
- 代码块支持一键复制
- 输入框支持拖拽附件

---

## 端到端使用示例

1. 选择一个模型
2. 输入问题并发送
3. 观察流式响应
4. 上传一张图片并提问
5. 上传文档并让模型总结
6. 使用语音输入补充问题
7. 在知识库中添加新知识
8. 再次提问，观察 RAG 加持效果

---

## 目录结构

- app.js：前端逻辑与渲染
- index.html：页面结构
- styles.css：样式
- server.py：本地服务与接口
- memory.db：RAG 知识库数据
- README.md：项目说明
- LICENSE：开源协议

---

## 常见问题

### 模型列表为空

- 确认 Ollama 服务已启动
- 检查模型是否已下载
- 尝试点击“刷新模型列表”

### RAG 状态未就绪

- 确认本地有 embedding 模型
- 等待首次检测完成
- 无 embedding 模型时 RAG 会显示不可用

### 语音输入不可用

- 需要浏览器支持 SpeechRecognition API
- 建议使用 Chrome 或 Edge

### 联网搜索不可用

- 依赖 `duckduckgo_search`
- 若未安装，联网搜索会失败

### 文件无法上传

- 单个文件大小上限为 10MB
- 可检查浏览器控制台是否有错误

---

## 安全与隐私

- 所有数据处理均在本地完成
- 不会主动上传对话到第三方服务
- 联网搜索仅在启用时触发外部请求

---

## 开发与扩展建议

- 可新增更多工具调用
- 可扩展多模态能力
- 可接入更多模型服务端点
- 可增加多语言 UI 支持

---

## 版本与兼容性说明

- 项目基于标准浏览器 API 与 Ollama HTTP 接口
- 若 Ollama API 发生更新，可能需要调整 server.py

---

## 许可

MIT License
